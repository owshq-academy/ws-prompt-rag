{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Chunking Strategies Demonstration with UberEats Dataset\n",
    "\n",
    "This notebook showcases and compares five different text chunking strategies using a real-world dataset document related to UberEats architecture. Chunking is a foundational step in Natural Language Processing (NLP), especially for tasks like embedding, retrieval, summarization, and LLM-based question answering.\n",
    "\n",
    "We will explore the following techniques:\n",
    "\n",
    "1. **Fixed-Size Chunking** — Splits the text into equal-sized character blocks.\n",
    "2. **Recursive Character Text Splitting** — Uses intelligent fallbacks to split text based on structure (paragraphs, lines, sentences).\n",
    "3. **Semantic Chunking** — Groups semantically related sentences using sentence embeddings.\n",
    "4. **Language-Based Chunking** — Splits by linguistic units (sentences, paragraphs, or words).\n",
    "5. **Context-Aware Chunking** — Applies sliding windows with overlap to preserve context across chunks.\n",
    "\n",
    "Each strategy has trade-offs in terms of **granularity**, **coherence**, and **suitability for downstream tasks** like Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "The source document is a detailed specification of the UberEats data architecture, ideal for chunking experiments due to its rich structure and vocabulary diversity."
   ],
   "id": "68e627b16da36254"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-26T04:30:26.322545Z",
     "start_time": "2025-07-26T04:29:11.081018Z"
    }
   },
   "source": [
    "!pip install pdfminer.six langchain sentence-transformers nltk scikit-learn --quiet\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/luanmorenomaciel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:30:27.613667Z",
     "start_time": "2025-07-26T04:30:26.691648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "pdf_path = \"data/doc-datasets.pdf\"\n",
    "text = extract_text(pdf_path)\n",
    "\n",
    "print(text[:1000])"
   ],
   "id": "571f8c9d0773764c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentação de Datasets: Caso de \n",
      "Uso UberEats \n",
      "\n",
      "Introdução \n",
      "\n",
      "Este documento fornece uma visão geral das entidades de negócio e suas respectivas fontes de \n",
      "dados, que serão utilizadas nos laboratórios práticos para desenvolver pipelines de dados \n",
      "utilizando Apache Spark e suas APIs. \n",
      "\n",
      "Visão Geral da Arquitetura de Dados \n",
      "\n",
      "Os dados estão distribuídos em múltiplos sistemas, simulando uma arquitetura típica de \n",
      "microserviços: \n",
      "\n",
      "●  PostgreSQL: Armazena dados relacionados a motoristas e inventário \n",
      "●  MySQL: Mantém informações sobre restaurantes, produtos, avaliações e menu \n",
      "●  MongoDB: Contém dados de usuários, itens, recomendações e tickets de suporte \n",
      "●  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \n",
      "\n",
      "rotas \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fEntidades de Negócio \n",
      "\n",
      "1. Usuários \n",
      "\n",
      "Representam os clientes que fazem pedidos na plataforma. \n",
      "\n",
      "Fontes: \n",
      "\n",
      "●  mongodb/users: Dados principais dos usuários \n",
      "●  mssql/users: Informações complementares e profissionais \n",
      "\n",
      "Atributos princ\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📏 **1) Fixed Size Chunking**\n",
    "\n",
    "This strategy splits the document into fixed-length character chunks, optionally with **overlap**, to preserve partial context across chunk boundaries. It’s a simple and commonly used baseline for many NLP tasks like retrieval and summarization.\n",
    "\n",
    "### 📌 How it works:\n",
    "- Divides the text into equal-sized blocks (e.g., 500 characters).\n",
    "- Optionally includes a fixed number of overlapping characters (e.g., 100) from the previous chunk to **maintain context**.\n",
    "- Does not consider sentence or paragraph structure.\n",
    "\n",
    "### ✅ Pros:\n",
    "- Very simple and fast to implement.\n",
    "- Useful when uniform chunk size is required (e.g., for token limits).\n",
    "\n",
    "### ⚠️ Cons:\n",
    "- Can break sentences and split semantic meaning.\n",
    "- Overlap helps, but still lacks semantic awareness.\n",
    "\n",
    "> ✅ Use when you need deterministic chunk sizes and simple logic, especially for testing or early prototypes of Retrieval-Augmented Generation (RAG) pipelines."
   ],
   "id": "e8a8e1a66be89c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:38:15.259039Z",
     "start_time": "2025-07-26T04:38:15.254804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fixed_size_chunking_with_overlap(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "fixed_chunks = fixed_size_chunking_with_overlap(text, chunk_size=500, overlap=100)\n",
    "print(f\"Total Fixed-Size Chunks with Overlap: {len(fixed_chunks)}\")\n",
    "fixed_chunks[:2]"
   ],
   "id": "b719d7db563cf5d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fixed-Size Chunks with Overlap: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Documentação de Datasets: Caso de \\nUso UberEats \\n\\nIntrodução \\n\\nEste documento fornece uma visão geral das entidades de negócio e suas respectivas fontes de \\ndados, que serão utilizadas nos laboratórios práticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs. \\n\\nVisão Geral da Arquitetura de Dados \\n\\nOs dados estão distribuídos em múltiplos sistemas, simulando uma arquitetura típica de \\nmicroserviços: \\n\\n●  PostgreSQL: Armazena dados relacionados a motoristas e inventário \\n',\n",
       " 'a típica de \\nmicroserviços: \\n\\n●  PostgreSQL: Armazena dados relacionados a motoristas e inventário \\n●  MySQL: Mantém informações sobre restaurantes, produtos, avaliações e menu \\n●  MongoDB: Contém dados de usuários, itens, recomendações e tickets de suporte \\n●  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \\n\\nrotas \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cEntidades de Negócio \\n\\n1. Usuários \\n\\nRepresentam os clientes que fazem pedidos na plataforma. \\n\\nFontes: \\n\\n●  mongodb/users: Dados p']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧩 **2) Recursive Character Text Splitting**\n",
    "\n",
    "This strategy is smarter than fixed-size chunking — it tries to split the text at semantically meaningful boundaries using a **priority list of separators**, like paragraphs, sentences, spaces, and characters.\n",
    "\n",
    "It's provided by LangChain’s `RecursiveCharacterTextSplitter`, which is designed for **language models** that benefit from coherent, context-preserving input chunks.\n",
    "\n",
    "### 📌 How it works:\n",
    "- Tries to split the text at the largest possible separator (e.g., `\\n\\n`, `\\n`, `.`, `\" \"`).\n",
    "- If the resulting chunk is too large, it **recursively uses smaller separators**.\n",
    "- Supports chunk overlap to preserve context.\n",
    "\n",
    "### ✅ Pros:\n",
    "- More natural and semantically meaningful chunks than fixed-size.\n",
    "- Preserves sentence boundaries where possible.\n",
    "- Great for long-form documents and LLM pre-processing.\n",
    "\n",
    "### ⚠️ Cons:\n",
    "- Slightly more complex than fixed-size.\n",
    "- Resulting chunk sizes may vary slightly depending on text structure.\n",
    "\n",
    "> ✅ Use this when you want chunks that maintain coherence without splitting in the middle of important sentences or paragraphs. Ideal for **RAG pipelines**, **summarization**, or **semantic search**."
   ],
   "id": "2a5d8c944b5b8102"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:39:09.581478Z",
     "start_time": "2025-07-26T04:39:09.570641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def recursive_char_split(text, chunk_size=500, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "recursive_chunks = recursive_char_split(text)\n",
    "print(f\"Total Recursive Chunks: {len(recursive_chunks)}\")\n",
    "recursive_chunks[:2]"
   ],
   "id": "1374aaaed831064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Recursive Chunks: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Documentação de Datasets: Caso de \\nUso UberEats \\n\\nIntrodução \\n\\nEste documento fornece uma visão geral das entidades de negócio e suas respectivas fontes de \\ndados, que serão utilizadas nos laboratórios práticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs. \\n\\nVisão Geral da Arquitetura de Dados \\n\\nOs dados estão distribuídos em múltiplos sistemas, simulando uma arquitetura típica de \\nmicroserviços:',\n",
       " '●  PostgreSQL: Armazena dados relacionados a motoristas e inventário \\n●  MySQL: Mantém informações sobre restaurantes, produtos, avaliações e menu \\n●  MongoDB: Contém dados de usuários, itens, recomendações e tickets de suporte \\n●  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \\n\\nrotas \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cEntidades de Negócio \\n\\n1. Usuários \\n\\nRepresentam os clientes que fazem pedidos na plataforma. \\n\\nFontes:']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 **3) Semantic Chunking (with SentenceTransformer)**\n",
    "\n",
    "This strategy groups sentences based on their **semantic meaning** rather than arbitrary character limits. By leveraging sentence embeddings, we can ensure each chunk holds cohesive and contextually related ideas.\n",
    "\n",
    "We use `SentenceTransformer` from the `sentence-transformers` library to embed sentences before forming chunks of `N` semantically grouped sentences.\n",
    "\n",
    "### 📌 How it works:\n",
    "- Splits the text into individual sentences using `nltk.sent_tokenize()`.\n",
    "- Embeds each sentence using a transformer model like `all-MiniLM-L6-v2`.\n",
    "- Groups every `N` sentences into a chunk (default is 5).\n",
    "- This ensures each chunk has coherent meaning and balanced length.\n",
    "\n",
    "### ✅ Pros:\n",
    "- Excellent semantic integrity — sentences in the same chunk are related.\n",
    "- Great for applications like summarization, document understanding, and RAG.\n",
    "- Respects natural language boundaries (sentences).\n",
    "\n",
    "### ⚠️ Cons:\n",
    "- Slightly heavier on computation (embeddings).\n",
    "- Chunk size is based on number of sentences, not characters/tokens — may be harder to control model input size.\n",
    "\n",
    "> ✅ Use this when meaning and topic preservation are more important than strict chunk size control. Excellent for **vector databases**, **semantic search**, and **context-aware LLM prompts**."
   ],
   "id": "978365cf763240fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:41:45.661086Z",
     "start_time": "2025-07-26T04:41:22.802818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "punkt_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def semantic_chunking(text, chunk_size=5):\n",
    "    sentences = punkt_tokenizer.tokenize(text)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(current_chunk) >= chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "semantic_chunks = semantic_chunking(text, chunk_size=5)\n",
    "print(f\"Total Semantic Chunks: {len(semantic_chunks)}\")\n",
    "semantic_chunks[:2]"
   ],
   "id": "ab99b6ebe14ca8eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/luanmorenomaciel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Batches:   0%|          | 0/2 [00:00<?, ?it/s]/Users/luanmorenomaciel/GitHub/ws-prompt-rag/src/cnk-emb/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Semantic Chunks: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Documentação de Datasets: Caso de \\nUso UberEats \\n\\nIntrodução \\n\\nEste documento fornece uma visão geral das entidades de negócio e suas respectivas fontes de \\ndados, que serão utilizadas nos laboratórios práticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs. Visão Geral da Arquitetura de Dados \\n\\nOs dados estão distribuídos em múltiplos sistemas, simulando uma arquitetura típica de \\nmicroserviços: \\n\\n●  PostgreSQL: Armazena dados relacionados a motoristas e inventário \\n●  MySQL: Mantém informações sobre restaurantes, produtos, avaliações e menu \\n●  MongoDB: Contém dados de usuários, itens, recomendações e tickets de suporte \\n●  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \\n\\nrotas \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cEntidades de Negócio \\n\\n1. Usuários \\n\\nRepresentam os clientes que fazem pedidos na plataforma. Fontes: \\n\\n●  mongodb/users: Dados principais dos usuários \\n●  mssql/users: Informações complementares e profissionais \\n\\nAtributos principais: \\n\\n●  user_id: Identificador único do usuário \\n●  cpf: Documento de identificação brasileiro \\n●  email: Endereço de email para contato \\n●  delivery_address: Endereço de entrega \\n●  phone_number: Número de telefone \\n●  country: País (predominantemente Brasil) \\n●  city: Cidade \\n\\n2. Restaurantes \\n\\nEstabelecimentos cadastrados que oferecem produtos para delivery.',\n",
       " 'Fonte: \\n\\n●  mysql/restaurants \\n\\nAtributos principais: \\n\\n●  restaurant_id: Identificador único do restaurante \\n●  name: Nome do estabelecimento \\n●  cnpj: Documento de identificação empresarial brasileiro \\n●  address: Endereço físico \\n●  cuisine_type: Tipo de culinária (ex: Italiana, Japonesa) \\n●  opening_time/closing_time: Horários de funcionamento \\n●  average_rating: Avaliação média \\n●  num_reviews: Número total de avaliações \\n\\n \\n \\n\\x0c3. Motoristas/Entregadores \\n\\nParceiros responsáveis pela entrega dos pedidos. Fonte: \\n\\n●  postgres/drivers \\n\\nAtributos principais: \\n\\n●  driver_id: Identificador único do motorista \\n●  first_name/last_name: Nome do motorista \\n●  license_number: Número da habilitação \\n●  vehicle_type: Tipo de veículo (Carro, Moto, Bicicleta, etc.) ●  vehicle_make/vehicle_model: Fabricante e modelo do veículo \\n●  vehicle_year: Ano do veículo \\n\\n4. Produtos \\n\\nItens disponíveis para pedido nos restaurantes.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 **4) Semantic Chunking (with SentenceTransformer)**\n",
    "\n",
    "This strategy groups semantically related **sentences** into chunks, preserving meaning more effectively than basic character-based methods.\n",
    "\n",
    "Instead of splitting by length, we split by **linguistic boundaries (sentences)** and use a **sentence transformer** model to embed the text. In this version, we group every `N` sentences into a chunk — a simplified semantic grouping — but embeddings can be later used for smarter strategies like clustering or similarity-based grouping.\n",
    "\n",
    "### 📌 How it works:\n",
    "- Tokenizes the document into sentences using NLTK's `PunktSentenceTokenizer`.\n",
    "- Embeds each sentence using a transformer model (e.g., `all-MiniLM-L6-v2`).\n",
    "- Groups every `N` sentences into a single chunk (e.g., 5 sentences per chunk).\n",
    "\n",
    "### ✅ Pros:\n",
    "- Preserves semantic structure and sentence integrity.\n",
    "- Good balance between simplicity and meaning preservation.\n",
    "- Works well for input into LLMs or vector databases.\n",
    "\n",
    "### ⚠️ Cons:\n",
    "- Current logic groups sentences statically by count, not semantic similarity.\n",
    "- Chunk length may vary in character/token size.\n",
    "\n",
    "> ✅ Use this when sentence-level integrity and topical coherence are more important than strict token control. It’s ideal for document indexing, semantic search, or long-context LLM input.\n",
    "\n",
    "🔧 **Next step (optional)**: Enhance this by using cosine similarity of embeddings to create *adaptive* semantic chunks, dynamically grouping similar sentences."
   ],
   "id": "9570733bed611ab8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:43:10.299977Z",
     "start_time": "2025-07-26T04:43:10.291467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def language_based_chunking(text, by=\"paragraph\"):\n",
    "    if by == \"sentence\":\n",
    "        return sent_tokenize(text)\n",
    "    elif by == \"word\":\n",
    "        return word_tokenize(text)\n",
    "    elif by == \"paragraph\":\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "        return paragraphs\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported chunk type\")\n",
    "\n",
    "lb_chunks = language_based_chunking(text, by=\"paragraph\")\n",
    "print(f\"Total Language-Based Chunks (Paragraphs): {len(lb_chunks)}\")\n",
    "lb_chunks[:5]"
   ],
   "id": "e971aa7d6d4e7b8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Language-Based Chunks (Paragraphs): 134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Documentação de Datasets: Caso de \\nUso UberEats',\n",
       " 'Introdução',\n",
       " 'Este documento fornece uma visão geral das entidades de negócio e suas respectivas fontes de \\ndados, que serão utilizadas nos laboratórios práticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs.',\n",
       " 'Visão Geral da Arquitetura de Dados',\n",
       " 'Os dados estão distribuídos em múltiplos sistemas, simulando uma arquitetura típica de \\nmicroserviços:']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧵 **5) Context-Aware Chunking (Sliding Window / Stride)**\n",
    "\n",
    "This strategy splits the text using a **sliding window** approach, where each chunk includes some content from the previous chunk. It is designed to **preserve context** across boundaries by adding overlap — ensuring smoother continuity between chunks.\n",
    "\n",
    "This is particularly useful for **long-form document processing**, where cutting off context between chunks can lead to degraded performance in tasks like summarization, retrieval, or LLM question answering.\n",
    "\n",
    "### 📌 How it works:\n",
    "- Text is split into words or tokens.\n",
    "- A **window of fixed length** (e.g., 600 words) is extracted.\n",
    "- The window then **slides forward by a smaller step** (e.g., 100 words).\n",
    "- The result: overlapping chunks that maintain forward and backward context.\n",
    "\n",
    "### ✅ Pros:\n",
    "- Maintains flow of thought between chunks.\n",
    "- Avoids hard cuts that break meaning.\n",
    "- Works great with models like GPT, Claude, Gemini for context-aware tasks.\n",
    "\n",
    "### ⚠️ Cons:\n",
    "- Produces **more chunks**, increasing memory/compute cost.\n",
    "- Chunks may include **redundant or repeated content**.\n",
    "\n",
    "> ✅ Use this when downstream tasks benefit from maintaining the full context, such as in **retrieval-augmented generation (RAG)**, **dialogue agents**, or **multi-turn summarization**."
   ],
   "id": "87d5027706544249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:44:04.526818Z",
     "start_time": "2025-07-26T04:44:04.514990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def context_aware_chunking(text, max_chunk_length=600, stride=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), stride):\n",
    "        chunk = words[i:i + max_chunk_length]\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "context_chunks = context_aware_chunking(text)\n",
    "print(f\"Total Context-Aware Chunks: {len(context_chunks)}\")\n",
    "context_chunks[:2]"
   ],
   "id": "c1c090f6a242beb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Context-Aware Chunks: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Documentação de Datasets: Caso de Uso UberEats Introdução Este documento fornece uma visão geral das entidades de negócio e suas respectivas fontes de dados, que serão utilizadas nos laboratórios práticos para desenvolver pipelines de dados utilizando Apache Spark e suas APIs. Visão Geral da Arquitetura de Dados Os dados estão distribuídos em múltiplos sistemas, simulando uma arquitetura típica de microserviços: ● PostgreSQL: Armazena dados relacionados a motoristas e inventário ● MySQL: Mantém informações sobre restaurantes, produtos, avaliações e menu ● MongoDB: Contém dados de usuários, itens, recomendações e tickets de suporte ● Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e rotas Entidades de Negócio 1. Usuários Representam os clientes que fazem pedidos na plataforma. Fontes: ● mongodb/users: Dados principais dos usuários ● mssql/users: Informações complementares e profissionais Atributos principais: ● user_id: Identificador único do usuário ● cpf: Documento de identificação brasileiro ● email: Endereço de email para contato ● delivery_address: Endereço de entrega ● phone_number: Número de telefone ● country: País (predominantemente Brasil) ● city: Cidade 2. Restaurantes Estabelecimentos cadastrados que oferecem produtos para delivery. Fonte: ● mysql/restaurants Atributos principais: ● restaurant_id: Identificador único do restaurante ● name: Nome do estabelecimento ● cnpj: Documento de identificação empresarial brasileiro ● address: Endereço físico ● cuisine_type: Tipo de culinária (ex: Italiana, Japonesa) ● opening_time/closing_time: Horários de funcionamento ● average_rating: Avaliação média ● num_reviews: Número total de avaliações 3. Motoristas/Entregadores Parceiros responsáveis pela entrega dos pedidos. Fonte: ● postgres/drivers Atributos principais: ● driver_id: Identificador único do motorista ● first_name/last_name: Nome do motorista ● license_number: Número da habilitação ● vehicle_type: Tipo de veículo (Carro, Moto, Bicicleta, etc.) ● vehicle_make/vehicle_model: Fabricante e modelo do veículo ● vehicle_year: Ano do veículo 4. Produtos Itens disponíveis para pedido nos restaurantes. Fontes: ● mysql/products: Catálogo geral de produtos ● mongodb/items: Instâncias de produtos em pedidos Atributos principais: ● product_id: Identificador único do produto ● name: Nome do produto ● price: Preço de venda ● unit_cost: Custo unitário ● restaurant_id: Restaurante ao qual o produto pertence ● cuisine_type: Tipo de culinária ● calories: Informação nutricional ● is_vegetarian/is_gluten_free: Características dietéticas 5. Pedidos Solicitações de entrega feitas pelos usuários. Fonte: ● kafka/orders Atributos principais: ● order_id: Identificador único do pedido ● user_key: Referência ao usuário ● restaurant_key: Referência ao restaurante ● driver_key: Referência ao motorista ● order_date: Data e hora do pedido ● total_amount: Valor total do pedido ● payment_key: Referência ao pagamento 6. Status de Pedidos Acompanhamento do ciclo de vida de cada pedido. Fonte: ● kafka/status Valores possíveis: ● Order Placed (Pedido Realizado) ● In Analysis (Em Análise) ● Accepted (Aceito) ● Preparing (Em Preparação) ● Ready for Pickup (Pronto para Retirada) ● Picked Up (Retirado) ● Out for Delivery (Em Rota de Entrega) ● Delivered (Entregue) ● Completed (Finalizado) 7. Pagamentos Transações financeiras associadas aos pedidos. Fonte: ● kafka/payments Atributos principais: ● payment_id: Identificador único do pagamento ● order_key: Referência ao pedido ● amount: Valor total cobrado ● method: Método de pagamento (Cartão, Boleto, Carteira digital) ● status: Status da transação (succeeded, failed, pending, refunded) ● currency: Moeda da transação ● card_brand: Bandeira do cartão ● provider: Provedor de pagamento (PayPal, Adyen, etc.) 8. Avaliações Feedback dos clientes sobre os restaurantes. Fonte: ● mysql/ratings Atributos principais: ● rating_id: Identificador único da avaliação ● restaurant_identifier: Referência ao restaurante ● rating: Pontuação (escala de 1-5) ● timestamp: Data e hora da avaliação 9. Menu Organização dos produtos em seções nos restaurantes. Fonte: ● mysql/menu Atributos principais: ● menu_section_id: Identificador único da seção ● restaurant_id: Restaurante ao qual a seção pertence ● name: Nome da seção (ex: Entradas, Pratos Principais) ● description: Descrição da seção ● active:',\n",
       " 'pagamentos, status, GPS e rotas Entidades de Negócio 1. Usuários Representam os clientes que fazem pedidos na plataforma. Fontes: ● mongodb/users: Dados principais dos usuários ● mssql/users: Informações complementares e profissionais Atributos principais: ● user_id: Identificador único do usuário ● cpf: Documento de identificação brasileiro ● email: Endereço de email para contato ● delivery_address: Endereço de entrega ● phone_number: Número de telefone ● country: País (predominantemente Brasil) ● city: Cidade 2. Restaurantes Estabelecimentos cadastrados que oferecem produtos para delivery. Fonte: ● mysql/restaurants Atributos principais: ● restaurant_id: Identificador único do restaurante ● name: Nome do estabelecimento ● cnpj: Documento de identificação empresarial brasileiro ● address: Endereço físico ● cuisine_type: Tipo de culinária (ex: Italiana, Japonesa) ● opening_time/closing_time: Horários de funcionamento ● average_rating: Avaliação média ● num_reviews: Número total de avaliações 3. Motoristas/Entregadores Parceiros responsáveis pela entrega dos pedidos. Fonte: ● postgres/drivers Atributos principais: ● driver_id: Identificador único do motorista ● first_name/last_name: Nome do motorista ● license_number: Número da habilitação ● vehicle_type: Tipo de veículo (Carro, Moto, Bicicleta, etc.) ● vehicle_make/vehicle_model: Fabricante e modelo do veículo ● vehicle_year: Ano do veículo 4. Produtos Itens disponíveis para pedido nos restaurantes. Fontes: ● mysql/products: Catálogo geral de produtos ● mongodb/items: Instâncias de produtos em pedidos Atributos principais: ● product_id: Identificador único do produto ● name: Nome do produto ● price: Preço de venda ● unit_cost: Custo unitário ● restaurant_id: Restaurante ao qual o produto pertence ● cuisine_type: Tipo de culinária ● calories: Informação nutricional ● is_vegetarian/is_gluten_free: Características dietéticas 5. Pedidos Solicitações de entrega feitas pelos usuários. Fonte: ● kafka/orders Atributos principais: ● order_id: Identificador único do pedido ● user_key: Referência ao usuário ● restaurant_key: Referência ao restaurante ● driver_key: Referência ao motorista ● order_date: Data e hora do pedido ● total_amount: Valor total do pedido ● payment_key: Referência ao pagamento 6. Status de Pedidos Acompanhamento do ciclo de vida de cada pedido. Fonte: ● kafka/status Valores possíveis: ● Order Placed (Pedido Realizado) ● In Analysis (Em Análise) ● Accepted (Aceito) ● Preparing (Em Preparação) ● Ready for Pickup (Pronto para Retirada) ● Picked Up (Retirado) ● Out for Delivery (Em Rota de Entrega) ● Delivered (Entregue) ● Completed (Finalizado) 7. Pagamentos Transações financeiras associadas aos pedidos. Fonte: ● kafka/payments Atributos principais: ● payment_id: Identificador único do pagamento ● order_key: Referência ao pedido ● amount: Valor total cobrado ● method: Método de pagamento (Cartão, Boleto, Carteira digital) ● status: Status da transação (succeeded, failed, pending, refunded) ● currency: Moeda da transação ● card_brand: Bandeira do cartão ● provider: Provedor de pagamento (PayPal, Adyen, etc.) 8. Avaliações Feedback dos clientes sobre os restaurantes. Fonte: ● mysql/ratings Atributos principais: ● rating_id: Identificador único da avaliação ● restaurant_identifier: Referência ao restaurante ● rating: Pontuação (escala de 1-5) ● timestamp: Data e hora da avaliação 9. Menu Organização dos produtos em seções nos restaurantes. Fonte: ● mysql/menu Atributos principais: ● menu_section_id: Identificador único da seção ● restaurant_id: Restaurante ao qual a seção pertence ● name: Nome da seção (ex: Entradas, Pratos Principais) ● description: Descrição da seção ● active: Indica se a seção está ativa 10. Inventário Controle de estoque dos produtos nos restaurantes. Fonte: ● postgres/inventory Atributos principais: ● stock_id: Identificador único do registro de estoque ● restaurant_id: Referência ao restaurante ● product_id: Referência ao produto ● quantity_available: Quantidade disponível em estoque ● last_updated: Data e hora da última atualização 11. Rotas Trajetos percorridos pelos entregadores. Fonte: ● kafka/route Atributos principais: ● route_id: Identificador único da rota ● order_id: Referência ao pedido ● driver_id: Referência ao motorista ● start_time/end_time: Horários de início e fim ● start_lat/start_lon: Coordenadas de origem ● end_lat/end_lon: Coordenadas de destino ● distance_km: Distância percorrida']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
