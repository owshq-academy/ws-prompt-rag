{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ§  Chunking Strategies Demonstration with UberEats Dataset\n",
    "\n",
    "This notebook showcases and compares five different text chunking strategies using a real-world dataset document related to UberEats architecture. Chunking is a foundational step in Natural Language Processing (NLP), especially for tasks like embedding, retrieval, summarization, and LLM-based question answering.\n",
    "\n",
    "We will explore the following techniques:\n",
    "\n",
    "1. **Fixed-Size Chunking** â€” Splits the text into equal-sized character blocks.\n",
    "2. **Recursive Character Text Splitting** â€” Uses intelligent fallbacks to split text based on structure (paragraphs, lines, sentences).\n",
    "3. **Semantic Chunking** â€” Groups semantically related sentences using sentence embeddings.\n",
    "4. **Language-Based Chunking** â€” Splits by linguistic units (sentences, paragraphs, or words).\n",
    "5. **Context-Aware Chunking** â€” Applies sliding windows with overlap to preserve context across chunks.\n",
    "\n",
    "Each strategy has trade-offs in terms of **granularity**, **coherence**, and **suitability for downstream tasks** like Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "The source document is a detailed specification of the UberEats data architecture, ideal for chunking experiments due to its rich structure and vocabulary diversity."
   ],
   "id": "68e627b16da36254"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-26T04:30:26.322545Z",
     "start_time": "2025-07-26T04:29:11.081018Z"
    }
   },
   "source": [
    "!pip install pdfminer.six langchain sentence-transformers nltk scikit-learn --quiet\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/luanmorenomaciel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:30:27.613667Z",
     "start_time": "2025-07-26T04:30:26.691648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "pdf_path = \"data/doc-datasets.pdf\"\n",
    "text = extract_text(pdf_path)\n",
    "\n",
    "print(text[:1000])"
   ],
   "id": "571f8c9d0773764c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentaÃ§Ã£o de Datasets: Caso de \n",
      "Uso UberEats \n",
      "\n",
      "IntroduÃ§Ã£o \n",
      "\n",
      "Este documento fornece uma visÃ£o geral das entidades de negÃ³cio e suas respectivas fontes de \n",
      "dados, que serÃ£o utilizadas nos laboratÃ³rios prÃ¡ticos para desenvolver pipelines de dados \n",
      "utilizando Apache Spark e suas APIs. \n",
      "\n",
      "VisÃ£o Geral da Arquitetura de Dados \n",
      "\n",
      "Os dados estÃ£o distribuÃ­dos em mÃºltiplos sistemas, simulando uma arquitetura tÃ­pica de \n",
      "microserviÃ§os: \n",
      "\n",
      "â—  PostgreSQL: Armazena dados relacionados a motoristas e inventÃ¡rio \n",
      "â—  MySQL: MantÃ©m informaÃ§Ãµes sobre restaurantes, produtos, avaliaÃ§Ãµes e menu \n",
      "â—  MongoDB: ContÃ©m dados de usuÃ¡rios, itens, recomendaÃ§Ãµes e tickets de suporte \n",
      "â—  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \n",
      "\n",
      "rotas \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fEntidades de NegÃ³cio \n",
      "\n",
      "1. UsuÃ¡rios \n",
      "\n",
      "Representam os clientes que fazem pedidos na plataforma. \n",
      "\n",
      "Fontes: \n",
      "\n",
      "â—  mongodb/users: Dados principais dos usuÃ¡rios \n",
      "â—  mssql/users: InformaÃ§Ãµes complementares e profissionais \n",
      "\n",
      "Atributos princ\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ“ **1) Fixed Size Chunking**\n",
    "\n",
    "This strategy splits the document into fixed-length character chunks, optionally with **overlap**, to preserve partial context across chunk boundaries. Itâ€™s a simple and commonly used baseline for many NLP tasks like retrieval and summarization.\n",
    "\n",
    "### ğŸ“Œ How it works:\n",
    "- Divides the text into equal-sized blocks (e.g., 500 characters).\n",
    "- Optionally includes a fixed number of overlapping characters (e.g., 100) from the previous chunk to **maintain context**.\n",
    "- Does not consider sentence or paragraph structure.\n",
    "\n",
    "### âœ… Pros:\n",
    "- Very simple and fast to implement.\n",
    "- Useful when uniform chunk size is required (e.g., for token limits).\n",
    "\n",
    "### âš ï¸ Cons:\n",
    "- Can break sentences and split semantic meaning.\n",
    "- Overlap helps, but still lacks semantic awareness.\n",
    "\n",
    "> âœ… Use when you need deterministic chunk sizes and simple logic, especially for testing or early prototypes of Retrieval-Augmented Generation (RAG) pipelines."
   ],
   "id": "e8a8e1a66be89c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:38:15.259039Z",
     "start_time": "2025-07-26T04:38:15.254804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fixed_size_chunking_with_overlap(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "fixed_chunks = fixed_size_chunking_with_overlap(text, chunk_size=500, overlap=100)\n",
    "print(f\"Total Fixed-Size Chunks with Overlap: {len(fixed_chunks)}\")\n",
    "fixed_chunks[:2]"
   ],
   "id": "b719d7db563cf5d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fixed-Size Chunks with Overlap: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DocumentaÃ§Ã£o de Datasets: Caso de \\nUso UberEats \\n\\nIntroduÃ§Ã£o \\n\\nEste documento fornece uma visÃ£o geral das entidades de negÃ³cio e suas respectivas fontes de \\ndados, que serÃ£o utilizadas nos laboratÃ³rios prÃ¡ticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs. \\n\\nVisÃ£o Geral da Arquitetura de Dados \\n\\nOs dados estÃ£o distribuÃ­dos em mÃºltiplos sistemas, simulando uma arquitetura tÃ­pica de \\nmicroserviÃ§os: \\n\\nâ—  PostgreSQL: Armazena dados relacionados a motoristas e inventÃ¡rio \\n',\n",
       " 'a tÃ­pica de \\nmicroserviÃ§os: \\n\\nâ—  PostgreSQL: Armazena dados relacionados a motoristas e inventÃ¡rio \\nâ—  MySQL: MantÃ©m informaÃ§Ãµes sobre restaurantes, produtos, avaliaÃ§Ãµes e menu \\nâ—  MongoDB: ContÃ©m dados de usuÃ¡rios, itens, recomendaÃ§Ãµes e tickets de suporte \\nâ—  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \\n\\nrotas \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cEntidades de NegÃ³cio \\n\\n1. UsuÃ¡rios \\n\\nRepresentam os clientes que fazem pedidos na plataforma. \\n\\nFontes: \\n\\nâ—  mongodb/users: Dados p']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ§© **2) Recursive Character Text Splitting**\n",
    "\n",
    "This strategy is smarter than fixed-size chunking â€” it tries to split the text at semantically meaningful boundaries using a **priority list of separators**, like paragraphs, sentences, spaces, and characters.\n",
    "\n",
    "It's provided by LangChainâ€™s `RecursiveCharacterTextSplitter`, which is designed for **language models** that benefit from coherent, context-preserving input chunks.\n",
    "\n",
    "### ğŸ“Œ How it works:\n",
    "- Tries to split the text at the largest possible separator (e.g., `\\n\\n`, `\\n`, `.`, `\" \"`).\n",
    "- If the resulting chunk is too large, it **recursively uses smaller separators**.\n",
    "- Supports chunk overlap to preserve context.\n",
    "\n",
    "### âœ… Pros:\n",
    "- More natural and semantically meaningful chunks than fixed-size.\n",
    "- Preserves sentence boundaries where possible.\n",
    "- Great for long-form documents and LLM pre-processing.\n",
    "\n",
    "### âš ï¸ Cons:\n",
    "- Slightly more complex than fixed-size.\n",
    "- Resulting chunk sizes may vary slightly depending on text structure.\n",
    "\n",
    "> âœ… Use this when you want chunks that maintain coherence without splitting in the middle of important sentences or paragraphs. Ideal for **RAG pipelines**, **summarization**, or **semantic search**."
   ],
   "id": "2a5d8c944b5b8102"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:39:09.581478Z",
     "start_time": "2025-07-26T04:39:09.570641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def recursive_char_split(text, chunk_size=500, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "recursive_chunks = recursive_char_split(text)\n",
    "print(f\"Total Recursive Chunks: {len(recursive_chunks)}\")\n",
    "recursive_chunks[:2]"
   ],
   "id": "1374aaaed831064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Recursive Chunks: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DocumentaÃ§Ã£o de Datasets: Caso de \\nUso UberEats \\n\\nIntroduÃ§Ã£o \\n\\nEste documento fornece uma visÃ£o geral das entidades de negÃ³cio e suas respectivas fontes de \\ndados, que serÃ£o utilizadas nos laboratÃ³rios prÃ¡ticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs. \\n\\nVisÃ£o Geral da Arquitetura de Dados \\n\\nOs dados estÃ£o distribuÃ­dos em mÃºltiplos sistemas, simulando uma arquitetura tÃ­pica de \\nmicroserviÃ§os:',\n",
       " 'â—  PostgreSQL: Armazena dados relacionados a motoristas e inventÃ¡rio \\nâ—  MySQL: MantÃ©m informaÃ§Ãµes sobre restaurantes, produtos, avaliaÃ§Ãµes e menu \\nâ—  MongoDB: ContÃ©m dados de usuÃ¡rios, itens, recomendaÃ§Ãµes e tickets de suporte \\nâ—  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \\n\\nrotas \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cEntidades de NegÃ³cio \\n\\n1. UsuÃ¡rios \\n\\nRepresentam os clientes que fazem pedidos na plataforma. \\n\\nFontes:']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ§  **3) Semantic Chunking (with SentenceTransformer)**\n",
    "\n",
    "This strategy groups sentences based on their **semantic meaning** rather than arbitrary character limits. By leveraging sentence embeddings, we can ensure each chunk holds cohesive and contextually related ideas.\n",
    "\n",
    "We use `SentenceTransformer` from the `sentence-transformers` library to embed sentences before forming chunks of `N` semantically grouped sentences.\n",
    "\n",
    "### ğŸ“Œ How it works:\n",
    "- Splits the text into individual sentences using `nltk.sent_tokenize()`.\n",
    "- Embeds each sentence using a transformer model like `all-MiniLM-L6-v2`.\n",
    "- Groups every `N` sentences into a chunk (default is 5).\n",
    "- This ensures each chunk has coherent meaning and balanced length.\n",
    "\n",
    "### âœ… Pros:\n",
    "- Excellent semantic integrity â€” sentences in the same chunk are related.\n",
    "- Great for applications like summarization, document understanding, and RAG.\n",
    "- Respects natural language boundaries (sentences).\n",
    "\n",
    "### âš ï¸ Cons:\n",
    "- Slightly heavier on computation (embeddings).\n",
    "- Chunk size is based on number of sentences, not characters/tokens â€” may be harder to control model input size.\n",
    "\n",
    "> âœ… Use this when meaning and topic preservation are more important than strict chunk size control. Excellent for **vector databases**, **semantic search**, and **context-aware LLM prompts**."
   ],
   "id": "978365cf763240fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:41:45.661086Z",
     "start_time": "2025-07-26T04:41:22.802818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "punkt_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def semantic_chunking(text, chunk_size=5):\n",
    "    sentences = punkt_tokenizer.tokenize(text)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(current_chunk) >= chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "semantic_chunks = semantic_chunking(text, chunk_size=5)\n",
    "print(f\"Total Semantic Chunks: {len(semantic_chunks)}\")\n",
    "semantic_chunks[:2]"
   ],
   "id": "ab99b6ebe14ca8eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/luanmorenomaciel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Batches:   0%|          | 0/2 [00:00<?, ?it/s]/Users/luanmorenomaciel/GitHub/ws-prompt-rag/src/cnk-emb/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Semantic Chunks: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DocumentaÃ§Ã£o de Datasets: Caso de \\nUso UberEats \\n\\nIntroduÃ§Ã£o \\n\\nEste documento fornece uma visÃ£o geral das entidades de negÃ³cio e suas respectivas fontes de \\ndados, que serÃ£o utilizadas nos laboratÃ³rios prÃ¡ticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs. VisÃ£o Geral da Arquitetura de Dados \\n\\nOs dados estÃ£o distribuÃ­dos em mÃºltiplos sistemas, simulando uma arquitetura tÃ­pica de \\nmicroserviÃ§os: \\n\\nâ—  PostgreSQL: Armazena dados relacionados a motoristas e inventÃ¡rio \\nâ—  MySQL: MantÃ©m informaÃ§Ãµes sobre restaurantes, produtos, avaliaÃ§Ãµes e menu \\nâ—  MongoDB: ContÃ©m dados de usuÃ¡rios, itens, recomendaÃ§Ãµes e tickets de suporte \\nâ—  Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e \\n\\nrotas \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cEntidades de NegÃ³cio \\n\\n1. UsuÃ¡rios \\n\\nRepresentam os clientes que fazem pedidos na plataforma. Fontes: \\n\\nâ—  mongodb/users: Dados principais dos usuÃ¡rios \\nâ—  mssql/users: InformaÃ§Ãµes complementares e profissionais \\n\\nAtributos principais: \\n\\nâ—  user_id: Identificador Ãºnico do usuÃ¡rio \\nâ—  cpf: Documento de identificaÃ§Ã£o brasileiro \\nâ—  email: EndereÃ§o de email para contato \\nâ—  delivery_address: EndereÃ§o de entrega \\nâ—  phone_number: NÃºmero de telefone \\nâ—  country: PaÃ­s (predominantemente Brasil) \\nâ—  city: Cidade \\n\\n2. Restaurantes \\n\\nEstabelecimentos cadastrados que oferecem produtos para delivery.',\n",
       " 'Fonte: \\n\\nâ—  mysql/restaurants \\n\\nAtributos principais: \\n\\nâ—  restaurant_id: Identificador Ãºnico do restaurante \\nâ—  name: Nome do estabelecimento \\nâ—  cnpj: Documento de identificaÃ§Ã£o empresarial brasileiro \\nâ—  address: EndereÃ§o fÃ­sico \\nâ—  cuisine_type: Tipo de culinÃ¡ria (ex: Italiana, Japonesa) \\nâ—  opening_time/closing_time: HorÃ¡rios de funcionamento \\nâ—  average_rating: AvaliaÃ§Ã£o mÃ©dia \\nâ—  num_reviews: NÃºmero total de avaliaÃ§Ãµes \\n\\n \\n \\n\\x0c3. Motoristas/Entregadores \\n\\nParceiros responsÃ¡veis pela entrega dos pedidos. Fonte: \\n\\nâ—  postgres/drivers \\n\\nAtributos principais: \\n\\nâ—  driver_id: Identificador Ãºnico do motorista \\nâ—  first_name/last_name: Nome do motorista \\nâ—  license_number: NÃºmero da habilitaÃ§Ã£o \\nâ—  vehicle_type: Tipo de veÃ­culo (Carro, Moto, Bicicleta, etc.) â—  vehicle_make/vehicle_model: Fabricante e modelo do veÃ­culo \\nâ—  vehicle_year: Ano do veÃ­culo \\n\\n4. Produtos \\n\\nItens disponÃ­veis para pedido nos restaurantes.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ§  **4) Semantic Chunking (with SentenceTransformer)**\n",
    "\n",
    "This strategy groups semantically related **sentences** into chunks, preserving meaning more effectively than basic character-based methods.\n",
    "\n",
    "Instead of splitting by length, we split by **linguistic boundaries (sentences)** and use a **sentence transformer** model to embed the text. In this version, we group every `N` sentences into a chunk â€” a simplified semantic grouping â€” but embeddings can be later used for smarter strategies like clustering or similarity-based grouping.\n",
    "\n",
    "### ğŸ“Œ How it works:\n",
    "- Tokenizes the document into sentences using NLTK's `PunktSentenceTokenizer`.\n",
    "- Embeds each sentence using a transformer model (e.g., `all-MiniLM-L6-v2`).\n",
    "- Groups every `N` sentences into a single chunk (e.g., 5 sentences per chunk).\n",
    "\n",
    "### âœ… Pros:\n",
    "- Preserves semantic structure and sentence integrity.\n",
    "- Good balance between simplicity and meaning preservation.\n",
    "- Works well for input into LLMs or vector databases.\n",
    "\n",
    "### âš ï¸ Cons:\n",
    "- Current logic groups sentences statically by count, not semantic similarity.\n",
    "- Chunk length may vary in character/token size.\n",
    "\n",
    "> âœ… Use this when sentence-level integrity and topical coherence are more important than strict token control. Itâ€™s ideal for document indexing, semantic search, or long-context LLM input.\n",
    "\n",
    "ğŸ”§ **Next step (optional)**: Enhance this by using cosine similarity of embeddings to create *adaptive* semantic chunks, dynamically grouping similar sentences."
   ],
   "id": "9570733bed611ab8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:43:10.299977Z",
     "start_time": "2025-07-26T04:43:10.291467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def language_based_chunking(text, by=\"paragraph\"):\n",
    "    if by == \"sentence\":\n",
    "        return sent_tokenize(text)\n",
    "    elif by == \"word\":\n",
    "        return word_tokenize(text)\n",
    "    elif by == \"paragraph\":\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "        return paragraphs\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported chunk type\")\n",
    "\n",
    "lb_chunks = language_based_chunking(text, by=\"paragraph\")\n",
    "print(f\"Total Language-Based Chunks (Paragraphs): {len(lb_chunks)}\")\n",
    "lb_chunks[:5]"
   ],
   "id": "e971aa7d6d4e7b8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Language-Based Chunks (Paragraphs): 134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DocumentaÃ§Ã£o de Datasets: Caso de \\nUso UberEats',\n",
       " 'IntroduÃ§Ã£o',\n",
       " 'Este documento fornece uma visÃ£o geral das entidades de negÃ³cio e suas respectivas fontes de \\ndados, que serÃ£o utilizadas nos laboratÃ³rios prÃ¡ticos para desenvolver pipelines de dados \\nutilizando Apache Spark e suas APIs.',\n",
       " 'VisÃ£o Geral da Arquitetura de Dados',\n",
       " 'Os dados estÃ£o distribuÃ­dos em mÃºltiplos sistemas, simulando uma arquitetura tÃ­pica de \\nmicroserviÃ§os:']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ§µ **5) Context-Aware Chunking (Sliding Window / Stride)**\n",
    "\n",
    "This strategy splits the text using a **sliding window** approach, where each chunk includes some content from the previous chunk. It is designed to **preserve context** across boundaries by adding overlap â€” ensuring smoother continuity between chunks.\n",
    "\n",
    "This is particularly useful for **long-form document processing**, where cutting off context between chunks can lead to degraded performance in tasks like summarization, retrieval, or LLM question answering.\n",
    "\n",
    "### ğŸ“Œ How it works:\n",
    "- Text is split into words or tokens.\n",
    "- A **window of fixed length** (e.g., 600 words) is extracted.\n",
    "- The window then **slides forward by a smaller step** (e.g., 100 words).\n",
    "- The result: overlapping chunks that maintain forward and backward context.\n",
    "\n",
    "### âœ… Pros:\n",
    "- Maintains flow of thought between chunks.\n",
    "- Avoids hard cuts that break meaning.\n",
    "- Works great with models like GPT, Claude, Gemini for context-aware tasks.\n",
    "\n",
    "### âš ï¸ Cons:\n",
    "- Produces **more chunks**, increasing memory/compute cost.\n",
    "- Chunks may include **redundant or repeated content**.\n",
    "\n",
    "> âœ… Use this when downstream tasks benefit from maintaining the full context, such as in **retrieval-augmented generation (RAG)**, **dialogue agents**, or **multi-turn summarization**."
   ],
   "id": "87d5027706544249"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T04:44:04.526818Z",
     "start_time": "2025-07-26T04:44:04.514990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def context_aware_chunking(text, max_chunk_length=600, stride=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), stride):\n",
    "        chunk = words[i:i + max_chunk_length]\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "context_chunks = context_aware_chunking(text)\n",
    "print(f\"Total Context-Aware Chunks: {len(context_chunks)}\")\n",
    "context_chunks[:2]"
   ],
   "id": "c1c090f6a242beb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Context-Aware Chunks: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DocumentaÃ§Ã£o de Datasets: Caso de Uso UberEats IntroduÃ§Ã£o Este documento fornece uma visÃ£o geral das entidades de negÃ³cio e suas respectivas fontes de dados, que serÃ£o utilizadas nos laboratÃ³rios prÃ¡ticos para desenvolver pipelines de dados utilizando Apache Spark e suas APIs. VisÃ£o Geral da Arquitetura de Dados Os dados estÃ£o distribuÃ­dos em mÃºltiplos sistemas, simulando uma arquitetura tÃ­pica de microserviÃ§os: â— PostgreSQL: Armazena dados relacionados a motoristas e inventÃ¡rio â— MySQL: MantÃ©m informaÃ§Ãµes sobre restaurantes, produtos, avaliaÃ§Ãµes e menu â— MongoDB: ContÃ©m dados de usuÃ¡rios, itens, recomendaÃ§Ãµes e tickets de suporte â— Apache Kafka: Gerencia streams de eventos como pedidos, pagamentos, status, GPS e rotas Entidades de NegÃ³cio 1. UsuÃ¡rios Representam os clientes que fazem pedidos na plataforma. Fontes: â— mongodb/users: Dados principais dos usuÃ¡rios â— mssql/users: InformaÃ§Ãµes complementares e profissionais Atributos principais: â— user_id: Identificador Ãºnico do usuÃ¡rio â— cpf: Documento de identificaÃ§Ã£o brasileiro â— email: EndereÃ§o de email para contato â— delivery_address: EndereÃ§o de entrega â— phone_number: NÃºmero de telefone â— country: PaÃ­s (predominantemente Brasil) â— city: Cidade 2. Restaurantes Estabelecimentos cadastrados que oferecem produtos para delivery. Fonte: â— mysql/restaurants Atributos principais: â— restaurant_id: Identificador Ãºnico do restaurante â— name: Nome do estabelecimento â— cnpj: Documento de identificaÃ§Ã£o empresarial brasileiro â— address: EndereÃ§o fÃ­sico â— cuisine_type: Tipo de culinÃ¡ria (ex: Italiana, Japonesa) â— opening_time/closing_time: HorÃ¡rios de funcionamento â— average_rating: AvaliaÃ§Ã£o mÃ©dia â— num_reviews: NÃºmero total de avaliaÃ§Ãµes 3. Motoristas/Entregadores Parceiros responsÃ¡veis pela entrega dos pedidos. Fonte: â— postgres/drivers Atributos principais: â— driver_id: Identificador Ãºnico do motorista â— first_name/last_name: Nome do motorista â— license_number: NÃºmero da habilitaÃ§Ã£o â— vehicle_type: Tipo de veÃ­culo (Carro, Moto, Bicicleta, etc.) â— vehicle_make/vehicle_model: Fabricante e modelo do veÃ­culo â— vehicle_year: Ano do veÃ­culo 4. Produtos Itens disponÃ­veis para pedido nos restaurantes. Fontes: â— mysql/products: CatÃ¡logo geral de produtos â— mongodb/items: InstÃ¢ncias de produtos em pedidos Atributos principais: â— product_id: Identificador Ãºnico do produto â— name: Nome do produto â— price: PreÃ§o de venda â— unit_cost: Custo unitÃ¡rio â— restaurant_id: Restaurante ao qual o produto pertence â— cuisine_type: Tipo de culinÃ¡ria â— calories: InformaÃ§Ã£o nutricional â— is_vegetarian/is_gluten_free: CaracterÃ­sticas dietÃ©ticas 5. Pedidos SolicitaÃ§Ãµes de entrega feitas pelos usuÃ¡rios. Fonte: â— kafka/orders Atributos principais: â— order_id: Identificador Ãºnico do pedido â— user_key: ReferÃªncia ao usuÃ¡rio â— restaurant_key: ReferÃªncia ao restaurante â— driver_key: ReferÃªncia ao motorista â— order_date: Data e hora do pedido â— total_amount: Valor total do pedido â— payment_key: ReferÃªncia ao pagamento 6. Status de Pedidos Acompanhamento do ciclo de vida de cada pedido. Fonte: â— kafka/status Valores possÃ­veis: â— Order Placed (Pedido Realizado) â— In Analysis (Em AnÃ¡lise) â— Accepted (Aceito) â— Preparing (Em PreparaÃ§Ã£o) â— Ready for Pickup (Pronto para Retirada) â— Picked Up (Retirado) â— Out for Delivery (Em Rota de Entrega) â— Delivered (Entregue) â— Completed (Finalizado) 7. Pagamentos TransaÃ§Ãµes financeiras associadas aos pedidos. Fonte: â— kafka/payments Atributos principais: â— payment_id: Identificador Ãºnico do pagamento â— order_key: ReferÃªncia ao pedido â— amount: Valor total cobrado â— method: MÃ©todo de pagamento (CartÃ£o, Boleto, Carteira digital) â— status: Status da transaÃ§Ã£o (succeeded, failed, pending, refunded) â— currency: Moeda da transaÃ§Ã£o â— card_brand: Bandeira do cartÃ£o â— provider: Provedor de pagamento (PayPal, Adyen, etc.) 8. AvaliaÃ§Ãµes Feedback dos clientes sobre os restaurantes. Fonte: â— mysql/ratings Atributos principais: â— rating_id: Identificador Ãºnico da avaliaÃ§Ã£o â— restaurant_identifier: ReferÃªncia ao restaurante â— rating: PontuaÃ§Ã£o (escala de 1-5) â— timestamp: Data e hora da avaliaÃ§Ã£o 9. Menu OrganizaÃ§Ã£o dos produtos em seÃ§Ãµes nos restaurantes. Fonte: â— mysql/menu Atributos principais: â— menu_section_id: Identificador Ãºnico da seÃ§Ã£o â— restaurant_id: Restaurante ao qual a seÃ§Ã£o pertence â— name: Nome da seÃ§Ã£o (ex: Entradas, Pratos Principais) â— description: DescriÃ§Ã£o da seÃ§Ã£o â— active:',\n",
       " 'pagamentos, status, GPS e rotas Entidades de NegÃ³cio 1. UsuÃ¡rios Representam os clientes que fazem pedidos na plataforma. Fontes: â— mongodb/users: Dados principais dos usuÃ¡rios â— mssql/users: InformaÃ§Ãµes complementares e profissionais Atributos principais: â— user_id: Identificador Ãºnico do usuÃ¡rio â— cpf: Documento de identificaÃ§Ã£o brasileiro â— email: EndereÃ§o de email para contato â— delivery_address: EndereÃ§o de entrega â— phone_number: NÃºmero de telefone â— country: PaÃ­s (predominantemente Brasil) â— city: Cidade 2. Restaurantes Estabelecimentos cadastrados que oferecem produtos para delivery. Fonte: â— mysql/restaurants Atributos principais: â— restaurant_id: Identificador Ãºnico do restaurante â— name: Nome do estabelecimento â— cnpj: Documento de identificaÃ§Ã£o empresarial brasileiro â— address: EndereÃ§o fÃ­sico â— cuisine_type: Tipo de culinÃ¡ria (ex: Italiana, Japonesa) â— opening_time/closing_time: HorÃ¡rios de funcionamento â— average_rating: AvaliaÃ§Ã£o mÃ©dia â— num_reviews: NÃºmero total de avaliaÃ§Ãµes 3. Motoristas/Entregadores Parceiros responsÃ¡veis pela entrega dos pedidos. Fonte: â— postgres/drivers Atributos principais: â— driver_id: Identificador Ãºnico do motorista â— first_name/last_name: Nome do motorista â— license_number: NÃºmero da habilitaÃ§Ã£o â— vehicle_type: Tipo de veÃ­culo (Carro, Moto, Bicicleta, etc.) â— vehicle_make/vehicle_model: Fabricante e modelo do veÃ­culo â— vehicle_year: Ano do veÃ­culo 4. Produtos Itens disponÃ­veis para pedido nos restaurantes. Fontes: â— mysql/products: CatÃ¡logo geral de produtos â— mongodb/items: InstÃ¢ncias de produtos em pedidos Atributos principais: â— product_id: Identificador Ãºnico do produto â— name: Nome do produto â— price: PreÃ§o de venda â— unit_cost: Custo unitÃ¡rio â— restaurant_id: Restaurante ao qual o produto pertence â— cuisine_type: Tipo de culinÃ¡ria â— calories: InformaÃ§Ã£o nutricional â— is_vegetarian/is_gluten_free: CaracterÃ­sticas dietÃ©ticas 5. Pedidos SolicitaÃ§Ãµes de entrega feitas pelos usuÃ¡rios. Fonte: â— kafka/orders Atributos principais: â— order_id: Identificador Ãºnico do pedido â— user_key: ReferÃªncia ao usuÃ¡rio â— restaurant_key: ReferÃªncia ao restaurante â— driver_key: ReferÃªncia ao motorista â— order_date: Data e hora do pedido â— total_amount: Valor total do pedido â— payment_key: ReferÃªncia ao pagamento 6. Status de Pedidos Acompanhamento do ciclo de vida de cada pedido. Fonte: â— kafka/status Valores possÃ­veis: â— Order Placed (Pedido Realizado) â— In Analysis (Em AnÃ¡lise) â— Accepted (Aceito) â— Preparing (Em PreparaÃ§Ã£o) â— Ready for Pickup (Pronto para Retirada) â— Picked Up (Retirado) â— Out for Delivery (Em Rota de Entrega) â— Delivered (Entregue) â— Completed (Finalizado) 7. Pagamentos TransaÃ§Ãµes financeiras associadas aos pedidos. Fonte: â— kafka/payments Atributos principais: â— payment_id: Identificador Ãºnico do pagamento â— order_key: ReferÃªncia ao pedido â— amount: Valor total cobrado â— method: MÃ©todo de pagamento (CartÃ£o, Boleto, Carteira digital) â— status: Status da transaÃ§Ã£o (succeeded, failed, pending, refunded) â— currency: Moeda da transaÃ§Ã£o â— card_brand: Bandeira do cartÃ£o â— provider: Provedor de pagamento (PayPal, Adyen, etc.) 8. AvaliaÃ§Ãµes Feedback dos clientes sobre os restaurantes. Fonte: â— mysql/ratings Atributos principais: â— rating_id: Identificador Ãºnico da avaliaÃ§Ã£o â— restaurant_identifier: ReferÃªncia ao restaurante â— rating: PontuaÃ§Ã£o (escala de 1-5) â— timestamp: Data e hora da avaliaÃ§Ã£o 9. Menu OrganizaÃ§Ã£o dos produtos em seÃ§Ãµes nos restaurantes. Fonte: â— mysql/menu Atributos principais: â— menu_section_id: Identificador Ãºnico da seÃ§Ã£o â— restaurant_id: Restaurante ao qual a seÃ§Ã£o pertence â— name: Nome da seÃ§Ã£o (ex: Entradas, Pratos Principais) â— description: DescriÃ§Ã£o da seÃ§Ã£o â— active: Indica se a seÃ§Ã£o estÃ¡ ativa 10. InventÃ¡rio Controle de estoque dos produtos nos restaurantes. Fonte: â— postgres/inventory Atributos principais: â— stock_id: Identificador Ãºnico do registro de estoque â— restaurant_id: ReferÃªncia ao restaurante â— product_id: ReferÃªncia ao produto â— quantity_available: Quantidade disponÃ­vel em estoque â— last_updated: Data e hora da Ãºltima atualizaÃ§Ã£o 11. Rotas Trajetos percorridos pelos entregadores. Fonte: â— kafka/route Atributos principais: â— route_id: Identificador Ãºnico da rota â— order_id: ReferÃªncia ao pedido â— driver_id: ReferÃªncia ao motorista â— start_time/end_time: HorÃ¡rios de inÃ­cio e fim â— start_lat/start_lon: Coordenadas de origem â— end_lat/end_lon: Coordenadas de destino â— distance_km: DistÃ¢ncia percorrida']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
