# Retrieval-Augmented Generation (RAG) Module

This folder implements a RAG pipeline over your processed invoice data, enabling natural-language queries against your invoices.

### üóÇ Folder Structure
```
src/RAG/
‚îú‚îÄ‚îÄ README.md          # RAG module documentation (this file)
‚îú‚îÄ‚îÄ configs.py         # Paths, embedding & vector-store settings
‚îú‚îÄ‚îÄ utils.py           # Helper: serialize JSON records to plain text
‚îú‚îÄ‚îÄ ingest.py          # Load JSON outputs as LangChain Documents
‚îú‚îÄ‚îÄ retriever.py       # Chunk, embed, and index documents (Pinecone/Chroma)
‚îî‚îÄ‚îÄ qa.py              # RetrievalQA pipeline for question answering
```
### üíæ Prerequisites
- Python 3.10+
- A virtual environment with project dependencies installed:

```sh
pip install -r requirements.txt
pip install langchain-pinecone pinecone
```

- An OpenAI API key with embedding access
- A Pinecone account & index (if using Pinecone)

### ‚öôÔ∏è Configuration (configs.py)

Set your parameters in configs.py or via environment variables:
```sh
# Path to processed JSON invoices
RAW_JSON_DIR = Path(__file__).parent.parent / "langchain" / "data" / "processed"

# Embedding model (OpenAI)
EMBEDDING_MODEL = "text-embedding-ada-002"

# Chunking for text splitter
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Pinecone settings
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "small-1536")
PINECONE_NAMESPACE = os.getenv("PINECONE_NAMESPACE", "invoices")

# (Chroma fallback)
VECTORDB_DIR = Path(__file__).parent / "vectordb"
```

Populate a .env at project root with:
```text
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_ENVIRONMENT=us-west1-gcp
PINECONE_INDEX_NAME=small-1536
```
#### üöÄ Ingestion (ingest.py)

Reads all JSON files generated by analyze_invoice.py and converts each to a Document:
```python
from langchain.schema import Document

def load_documents() -> list[Document]:
    docs = []
    for file in RAW_JSON_DIR.glob('*.json'):
        record = json.loads(file.read_text())
        text = json_to_text(record)
        docs.append(Document(page_content=text, metadata={'source': str(file)}))
    return docs
```
### üîç Retriever (retriever.py)

Builds a vector retriever by:
1.	Loading documents via load_documents()
2.	Chunking text with CharacterTextSplitter
3.	Embedding chunks (always uses OpenAI embeddings)
4.	Indexing into Pinecone (or Chroma fallback)

Usage:
```python
from src.RAG.retriever import build_retriever
retriever = build_retriever(use_pinecone=True)  # or False for local Chroma
```
### üß† QA Pipeline (qa.py)

Exposes a simple RetrievalQA chain for end-user queries:
```python
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from src.RAG.retriever import build_retriever

llm = ChatOpenAI(model_name="gpt-4", temperature=0)
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=build_retriever())
answer = qa.run("What was the total amount on invoice 12345?")
print(answer)
```

### üìà Usage Example
1.	Make sure your processed JSON invoices are in src/langchain/data/processed/.
2.	Build or refresh the index:

```sh
python -c "from src.RAG.retriever import build_retriever; build_retriever()"
```

3.	Ask questions via the QA module:

```sh
python -c "from src.RAG.qa import run_qa; print(run_qa('List all invoice dates'))"
```


üõ†Ô∏è Next Steps
- Add custom templates in prompts.py to format queries or post-process answers.
- Integrate in a CLI or web service for interactive querying.
- Experiment with different chunk sizes or embedding models.
- Implement caching of LLM calls for cost savings.

This RAG module provides a foundation to query your invoice data at scale.